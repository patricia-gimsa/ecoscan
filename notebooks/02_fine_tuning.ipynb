{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v_57VxmFj_zb","executionInfo":{"status":"ok","timestamp":1755947793537,"user_tz":-120,"elapsed":22099,"user":{"displayName":"Patricia Gim√©nez","userId":"16444508660089000781"}},"outputId":"2de4b2b5-948b-4aeb-ca29-5af55b7a3280"},"id":"v_57VxmFj_zb","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Create local folder where the dataset will be extracted\n","!mkdir -p /content/data"],"metadata":{"id":"u56jKD5j4XPc","executionInfo":{"status":"ok","timestamp":1755948022180,"user_tz":-120,"elapsed":114,"user":{"displayName":"Patricia Gim√©nez","userId":"16444508660089000781"}}},"id":"u56jKD5j4XPc","execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Unzip dataset from Google Drive into Colab local storage\n","!unzip -q \"/content/drive/MyDrive/ecoscan/data/garbage_classification.zip\" -d /content/data/"],"metadata":{"id":"eHP64e73662q","executionInfo":{"status":"ok","timestamp":1755948051192,"user_tz":-120,"elapsed":14345,"user":{"displayName":"Patricia Gim√©nez","userId":"16444508660089000781"}}},"id":"eHP64e73662q","execution_count":3,"outputs":[]},{"cell_type":"code","source":["!pip install -q transformers accelerate datasets evaluate scikit-learn pillow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LdmZfhWfkiNw","executionInfo":{"status":"ok","timestamp":1755935746642,"user_tz":-120,"elapsed":7805,"user":{"displayName":"Patricia Gim√©nez","userId":"16444508660089000781"}},"outputId":"7e34104f-7a97-4cbd-e0b3-dbb46b38271a"},"id":"LdmZfhWfkiNw","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["import torch, transformers, sklearn, PIL\n","print(\"CUDA:\", torch.cuda.is_available())\n","print(\"Transformers:\", transformers.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jijeATPmkpja","executionInfo":{"status":"ok","timestamp":1755935757054,"user_tz":-120,"elapsed":6503,"user":{"displayName":"Patricia Gim√©nez","userId":"16444508660089000781"}},"outputId":"f48bfd93-6a2d-466c-ed6a-1e9e9184cc76"},"id":"jijeATPmkpja","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA: True\n","Transformers: 4.55.2\n"]}]},{"cell_type":"markdown","id":"e0a8f738-ccc1-4dea-8474-4653432a166e","metadata":{"id":"e0a8f738-ccc1-4dea-8474-4653432a166e"},"source":["**Goal**: Set up imports, reproducibility, device (CPU/GPU), and project paths for training and model outputs."]},{"cell_type":"code","execution_count":null,"id":"29cf7785-442e-428b-8a55-e7442a68272b","metadata":{"id":"29cf7785-442e-428b-8a55-e7442a68272b","outputId":"48742beb-7881-42b7-f357-ae1823a9351c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755936102374,"user_tz":-120,"elapsed":13698,"user":{"displayName":"Patricia Gim√©nez","userId":"16444508660089000781"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n","GPU: NVIDIA A100-SXM4-40GB\n","DATA_DIR: /content/data/garbage_classification\n","OUT_DIR : /content/drive/MyDrive/ecoscan/models/vit_ecoscan_v1\n"]}],"source":["# Imports & basic setup\n","import os, json, random\n","import pandas as pd\n","from pathlib import Path\n","import numpy as np\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import (\n","    AutoImageProcessor,\n","    TrainingArguments,\n","    Trainer,\n","    ViTForImageClassification,\n",")\n","from transformers.trainer_callback import EarlyStoppingCallback\n","from transformers.trainer_utils import IntervalStrategy, SaveStrategy\n","\n","# Reproducibility\n","SEED = 42\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","\n","# (Optional) make CUDA runs more deterministic\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","# Device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device:\", device)\n","if torch.cuda.is_available():\n","    print(\"GPU:\", torch.cuda.get_device_name(0))\n","\n","# Paths\n","# Dataset lives in Colab local storage (fast)\n","DATA_DIR = Path(\"/content/data/garbage_classification\")\n","\n","# Outputs (checkpoints, final model, metadata) go to Google Drive\n","ROOT_DRIVE = Path(\"/content/drive/MyDrive/ecoscan\")\n","OUT_DIR = ROOT_DRIVE / \"models\" / \"vit_ecoscan_v1\"\n","OUT_DIR.mkdir(parents=True, exist_ok=True)\n","\n","print(\"DATA_DIR:\", DATA_DIR.resolve())\n","print(\"OUT_DIR :\", OUT_DIR.resolve())\n","\n","# Safety check: fail early if DATA_DIR does not exist\n","assert DATA_DIR.exists(), f\"Dataset folder not found at {DATA_DIR}. Check your path.\"\n","\n"]},{"cell_type":"markdown","id":"8cd7e6d7-84f6-4b75-95b6-6051363e906c","metadata":{"id":"8cd7e6d7-84f6-4b75-95b6-6051363e906c"},"source":["**Goal**: Scan dataset folders (one class per folder), count images per class, and create a stratified 80/20 train/validation split"]},{"cell_type":"code","execution_count":null,"id":"ceacd3c3-8ce2-41c5-9cbc-c73a4eb0b7b0","metadata":{"id":"ceacd3c3-8ce2-41c5-9cbc-c73a4eb0b7b0","outputId":"84927b1c-55cb-4e8a-c526-589761171a3a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755936658932,"user_tz":-120,"elapsed":494,"user":{"displayName":"Patricia Gim√©nez","userId":"16444508660089000781"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Classes: ['battery', 'brown-glass', 'cardboard', 'clothes', 'electronics', 'green-glass', 'metal_packaging', 'oil', 'organic', 'paper', 'plastic', 'shoes', 'tetrapak', 'trash', 'white-glass'] \n","Num classes: 15\n","Total images found: 16022\n","Train: 12817 images | Val: 3205 images\n","Train per class: {'battery': 756, 'brown-glass': 486, 'cardboard': 713, 'clothes': 4260, 'electronics': 156, 'green-glass': 503, 'metal_packaging': 615, 'oil': 146, 'organic': 788, 'paper': 840, 'plastic': 692, 'shoes': 1581, 'tetrapak': 103, 'trash': 558, 'white-glass': 620}\n","Val  per class : {'battery': 189, 'brown-glass': 121, 'cardboard': 178, 'clothes': 1065, 'electronics': 39, 'green-glass': 126, 'metal_packaging': 154, 'oil': 37, 'organic': 197, 'paper': 210, 'plastic': 173, 'shoes': 396, 'tetrapak': 26, 'trash': 139, 'white-glass': 155}\n"]}],"source":["# Scan dataset & create stratified train/val split\n","IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n","IGNORE_DIRS = {'.ipynb_checkpoints', '__MACOSX'}\n","\n","# List class names from subfolders\n","classes = sorted([d.name for d in DATA_DIR.iterdir() if d.is_dir() and d.name not in IGNORE_DIRS])\n","print(\"Classes:\", classes, \"\\nNum classes:\", len(classes))\n","\n","# Mappings for later use in Trainer\n","label2id = {name: i for i, name in enumerate(classes)}\n","id2label = {i: name for i, name in enumerate(classes)}\n","\n","# Collect all (image_path, class_id) pairs\n","samples = []\n","for cid, cname in enumerate(classes):\n","    class_dir = DATA_DIR / cname\n","    for p in class_dir.rglob(\"*\"):\n","        if p.is_file() and p.suffix.lower() in IMG_EXTS:\n","            samples.append((p, cid))  # keep as Path for now\n","\n","print(\"Total images found:\", len(samples))\n","assert len(samples) > 0, \"No images found. Check your dataset folder structure.\"\n","\n","# (Optional) ensure each class has enough samples for stratify\n","import collections\n","raw_counts = collections.Counter([cid for _, cid in samples])\n","too_small = {classes[k]: v for k, v in raw_counts.items() if v < 2}\n","assert not too_small, f\"Some classes have <2 images (needed for stratify): {too_small}\"\n","\n","# Build lists for split\n","paths  = [s[0] for s in samples]\n","labels = [s[1] for s in samples]\n","\n","# Stratified split to keep class balance\n","train_paths, val_paths, y_train, y_val = train_test_split(\n","    paths, labels, test_size=0.2, random_state=SEED, stratify=labels\n",")\n","\n","print(f\"Train: {len(train_paths)} images | Val: {len(val_paths)} images\")\n","\n","# Per-class counts (sanity check)\n","def count_by_class(lbls):\n","    c = collections.Counter(lbls)\n","    return {classes[k]: v for k, v in sorted(c.items())}\n","\n","print(\"Train per class:\", count_by_class(y_train))\n","print(\"Val  per class :\", count_by_class(y_val))\n"]},{"cell_type":"code","execution_count":null,"id":"ded5b12d-1406-408b-a816-cafc72e365fe","metadata":{"id":"ded5b12d-1406-408b-a816-cafc72e365fe","outputId":"edd76e13-ee20-4b5c-aeb1-644b7abfd404","colab":{"base_uri":"https://localhost:8080/","height":220,"referenced_widgets":["2b7792def8fb464fa8b579a11fa511dc","361144578f3243a5895be2b95e464105","b45b3fd6534f4770a40b1409f67e0344","866eabe4f0b7454daf43a955bfa3695f","73add9c8095c4f7e89fbd68c5ab93aea","1f49f69730ef45a982c12237657054f8","4acb64e89bca42ed8a47d46b23214178","db95e9c7c0494599a6db90fcd9e857ce","13e83674f40e489b99cfa08f907f4777","907de738b82a489baeaf633a3d6bb42a","12bf0c1b128941ccaafbf427840bf00f","37579880a8c14a499704231e773693e7","f14edd69efae4876ae91ca146e5b875e","f16532bfaa12486cb35d3ac852b1d6c7","5b1a17b7217a4643944b24d0870aaa17","295e7b7b90844873b0202d9a37cc1a44","a3e42ea8b6be4f3caabd52cca49885f6","46da212eed214e6ab229022ba69628f0","f9eaa19474e5424d875b117a0881e105","bb1a3b4a5c5849f89e426cae7614cb1e","8cdc0778c427454491cd45eeb151c831","5ad5757dce4e45b6b78cf7cf5d17ca9f"]},"executionInfo":{"status":"ok","timestamp":1755936886712,"user_tz":-120,"elapsed":1900,"user":{"displayName":"Patricia Gim√©nez","userId":"16444508660089000781"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b7792def8fb464fa8b579a11fa511dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37579880a8c14a499704231e773693e7"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Sample keys: dict_keys(['pixel_values', 'labels'])\n","pixel_values shape: torch.Size([3, 224, 224]) | label: 14\n"]}],"source":["# ViT processor, label maps (reused), and Dataset\n","\n","MODEL_NAME = \"google/vit-base-patch16-224\"\n","\n","# Processor: resize/normalize to ViT format (224x224, mean/std)\n","# use_fast=True evita el warning y acelera la preparaci√≥n\n","processor = AutoImageProcessor.from_pretrained(MODEL_NAME, use_fast=True)\n","\n","# Reuse label maps created in cell 2:\n","# classes, label2id, id2label  <-- ya existen\n","\n","# Save label maps (useful for inference/app)\n","(OUT_DIR / \"meta\").mkdir(parents=True, exist_ok=True)\n","with open(OUT_DIR / \"meta\" / \"id2label.json\", \"w\") as f:\n","    json.dump(id2label, f, indent=2)\n","with open(OUT_DIR / \"meta\" / \"label2id.json\", \"w\") as f:\n","    json.dump(label2id, f, indent=2)\n","\n","# Custom Dataset: reads a path, opens image, applies processor, returns tensors\n","class EcoScanDataset(torch.utils.data.Dataset):\n","    def __init__(self, paths, labels, processor, train=False):\n","        self.paths = paths          # list[Path|str]\n","        self.labels = labels        # list[int]\n","        self.processor = processor\n","        self.train = train\n","\n","    def __len__(self):\n","        return len(self.paths)\n","\n","    def __getitem__(self, idx):\n","        # Open image (Path or str is fine)\n","        img = Image.open(self.paths[idx]).convert(\"RGB\")\n","\n","        # Light augmentation for train only\n","        if self.train:\n","            if np.random.rand() < 0.5:\n","                img = img.transpose(Image.FLIP_LEFT_RIGHT)\n","            if np.random.rand() < 0.3:\n","                w, h = img.size\n","                crop = int(min(w, h) * 0.9)  # keep ~90% area\n","                left = np.random.randint(0, w - crop + 1)\n","                top  = np.random.randint(0, h - crop + 1)\n","                img = img.crop((left, top, left + crop, top + crop)).resize((w, h))\n","\n","        # Processor -> pixel_values tensor (includes resize to 224 and normalize)\n","        enc = self.processor(images=img, return_tensors=\"pt\")\n","        item = {k: v.squeeze(0) for k, v in enc.items()}  # remove batch dim\n","        item[\"labels\"] = torch.tensor(self.labels[idx]).long()\n","        return item\n","\n","# Build train/val datasets\n","train_ds = EcoScanDataset(train_paths, y_train, processor, train=True)\n","val_ds   = EcoScanDataset(val_paths,   y_val,   processor, train=False)\n","\n","# Sanity check\n","sample = train_ds[0]\n","print(\"Sample keys:\", sample.keys())\n","print(\"pixel_values shape:\", sample[\"pixel_values\"].shape, \"| label:\", sample[\"labels\"].item())\n","\n"]},{"cell_type":"markdown","id":"a10e7cb1-d3ce-4565-b8fe-1f60aa2db64e","metadata":{"id":"a10e7cb1-d3ce-4565-b8fe-1f60aa2db64e"},"source":["This cell prepares the dataset so the Vision Transformer can read the images. The processor resizes each image to 224√ó224, normalizes colors, and turns it into numbers (tensors). The dictionaries id2label and label2id just translate between class names (like ‚Äúplastic‚Äù) and numbers (like 12). The custom Dataset class loads each image, applies the processor, and adds the correct label. We then create two datasets: one for training and one for validation. Finally, checking a sample shows that each item has pixel_values (the image as numbers) and labels (the class ID). The shape [3, 224, 224] simply means 3 color channels (RGB) and an image size of 224√ó224."]},{"cell_type":"markdown","id":"6565a62e-98ed-4aaf-80bc-6134a826a955","metadata":{"id":"6565a62e-98ed-4aaf-80bc-6134a826a955"},"source":["**Goal**: Prepare DataLoaders so the model can efficiently iterate over training and validation samples in mini-batches"]},{"cell_type":"code","execution_count":null,"id":"60f3ea90-bce5-42a5-a89c-29e98fc31cb6","metadata":{"id":"60f3ea90-bce5-42a5-a89c-29e98fc31cb6","outputId":"50d1d9a2-c209-4cec-dd25-ca54f71c91c4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755936943250,"user_tz":-120,"elapsed":81,"user":{"displayName":"Patricia Gim√©nez","userId":"16444508660089000781"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Batch pixel_values shape: torch.Size([16, 3, 224, 224])\n","Batch labels shape     : torch.Size([16])\n"]}],"source":["# Quick sanity check: batch shapes from the training dataset\n","BATCH_SIZE = 16  # small if training on CPU\n","\n","tmp_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n","batch = next(iter(tmp_loader))\n","print(\"Batch pixel_values shape:\", batch[\"pixel_values\"].shape)  # [B, 3, 224, 224]\n","print(\"Batch labels shape     :\", batch[\"labels\"].shape)         # [B]\n","del tmp_loader  # not needed by HuggingFace Trainer\n"]},{"cell_type":"markdown","id":"5668453e-61fa-42e0-9fa8-e4dbe46d0e48","metadata":{"id":"5668453e-61fa-42e0-9fa8-e4dbe46d0e48"},"source":[" This makes training faster and easier to handle on CPU. The train_loader also shuffles the images so the model does not memorize the order. The final print shows that each batch has shape [B, 3, 224, 224] for the images (B = batch size, 16) and [B] for the labels."]},{"cell_type":"markdown","id":"290f95f8-599b-4de2-b27d-1b9307b1549e","metadata":{"id":"290f95f8-599b-4de2-b27d-1b9307b1549e"},"source":["**Goal**: Load a pre-trained ViT model and adapt it for our number of classes."]},{"cell_type":"code","execution_count":null,"id":"ceecfd73-6438-41f0-9237-f36c89ed4ea5","metadata":{"id":"ceecfd73-6438-41f0-9237-f36c89ed4ea5","outputId":"30d4f3eb-b0d8-4283-d132-3497e2d4872d","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["efc9d02993d04506a65b5c5d66c8876a","18912818b88a4cd8af79384056a22407","22d916c4513e4335a7152171906e1b67","1fc270f5a9f446caacb2b2ee4618dc31","638d8bd65e0e4e98a9b68f0563a1fd8e","1e3feeb8377e400bbb38cbb6259737c0","1cb99b7ce37a4cb5a912f574406180f6","c6a6bfff2ed24292b00d95a54f8ff846","f20ea386a59b4300ae4233611dd55030","5073a28500dc4a0f852d30f3f58b96d9","94d2041e8f9645d18eebb6d8cd5355dc"]},"executionInfo":{"status":"ok","timestamp":1755936976579,"user_tz":-120,"elapsed":2354,"user":{"displayName":"Patricia Gim√©nez","userId":"16444508660089000781"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efc9d02993d04506a65b5c5d66c8876a"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n","- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([15]) in the model instantiated\n","- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([15, 768]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["ViTConfig {\n","  \"architectures\": [\n","    \"ViTForImageClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"encoder_stride\": 16,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"battery\",\n","    \"1\": \"brown-glass\",\n","    \"2\": \"cardboard\",\n","    \"3\": \"clothes\",\n","    \"4\": \"electronics\",\n","    \"5\": \"green-glass\",\n","    \"6\": \"metal_packaging\",\n","    \"7\": \"oil\",\n","    \"8\": \"organic\",\n","    \"9\": \"paper\",\n","    \"10\": \"plastic\",\n","    \"11\": \"shoes\",\n","    \"12\": \"tetrapak\",\n","    \"13\": \"trash\",\n","    \"14\": \"white-glass\"\n","  },\n","  \"image_size\": 224,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"battery\": 0,\n","    \"brown-glass\": 1,\n","    \"cardboard\": 2,\n","    \"clothes\": 3,\n","    \"electronics\": 4,\n","    \"green-glass\": 5,\n","    \"metal_packaging\": 6,\n","    \"oil\": 7,\n","    \"organic\": 8,\n","    \"paper\": 9,\n","    \"plastic\": 10,\n","    \"shoes\": 11,\n","    \"tetrapak\": 12,\n","    \"trash\": 13,\n","    \"white-glass\": 14\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"model_type\": \"vit\",\n","  \"num_attention_heads\": 12,\n","  \"num_channels\": 3,\n","  \"num_hidden_layers\": 12,\n","  \"patch_size\": 16,\n","  \"pooler_act\": \"tanh\",\n","  \"pooler_output_size\": 768,\n","  \"qkv_bias\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.55.2\"\n","}\n","\n"]}],"source":["\n","# Load pre-trained ViT and replace the classification head with ours\n","model = ViTForImageClassification.from_pretrained(\n","    MODEL_NAME,\n","    num_labels=len(classes),     # our number of classes\n","    id2label=id2label,           # id -> class\n","    label2id=label2id,           # class -> id\n","    ignore_mismatched_sizes=True\n",")\n","\n","# Print model config to check it worked\n","print(model.config)\n"]},{"cell_type":"markdown","id":"053c4795-f4dd-4a01-84f0-cce3306b8c18","metadata":{"id":"053c4795-f4dd-4a01-84f0-cce3306b8c18"},"source":["This cell loads the Vision Transformer (ViT) model pre-trained on ImageNet, but adapts it to our custom dataset. By default, the original model has a classification head with 1000 output neurons (for 1000 ImageNet classes). Since our dataset only has 15 classes, we use the parameter ignore_mismatched_sizes=True to tell Hugging Face to discard the old head and create a new one with the correct number of classes. This way, we can fine-tune the pre-trained features of ViT on our task without dimension mismatches"]},{"cell_type":"markdown","id":"e1a8c9ea-b6a1-4228-bb96-79c11a4b6f82","metadata":{"id":"e1a8c9ea-b6a1-4228-bb96-79c11a4b6f82"},"source":["**Goal**: Define training arguments for fine-tuning our ViT model"]},{"cell_type":"code","execution_count":null,"id":"79d9f7bd-1f3d-48b7-8bc5-5608f17bfd9b","metadata":{"id":"79d9f7bd-1f3d-48b7-8bc5-5608f17bfd9b","outputId":"d3ed6c97-0d7a-499f-a5e9-21bc475d7a0a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755937172710,"user_tz":-120,"elapsed":184,"user":{"displayName":"Patricia Gim√©nez","userId":"16444508660089000781"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["TrainingArguments ready ‚Üí IntervalStrategy.EPOCH SaveStrategy.EPOCH\n","Checkpoints will be saved to: /content/drive/MyDrive/ecoscan/models/vit_ecoscan_v1\n"]}],"source":["# Training Arguments (A100-optimized; save to Drive OUT_DIR defined above)\n","\n","BATCH_SIZE = 64  # try 64 on A100 (fall back to 32 if you get OOM)\n","\n","training_args = TrainingArguments(\n","    output_dir=str(OUT_DIR),                 # checkpoints in Drive\n","    logging_dir=str(OUT_DIR / \"logs\"),       # logs in Drive\n","\n","    # Eval / saving once per epoch\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    save_total_limit=2,                      # keep last 2 checkpoints\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"accuracy\",\n","    greater_is_better=True,\n","\n","    # Core hyperparameters\n","    num_train_epochs=3,\n","    per_device_train_batch_size=BATCH_SIZE,\n","    per_device_eval_batch_size=BATCH_SIZE,\n","    learning_rate=5e-5,\n","    weight_decay=0.01,\n","    warmup_ratio=0.05,                       # small LR warmup helps stability\n","\n","    # Performance tweaks for A100\n","    bf16=True,                               # A100 supports bfloat16 ‚Üí faster/safer than fp16\n","    dataloader_pin_memory=True,\n","    dataloader_num_workers=4,                # speed up data loading\n","    logging_steps=50,                        # fewer log writes ‚Üí less overhead\n","    report_to=\"none\",\n","    seed=SEED,\n","    save_safetensors=True,                   # smaller/safer checkpoint files\n",")\n","\n","print(\"TrainingArguments ready ‚Üí\", training_args.eval_strategy, training_args.save_strategy)\n","print(\"Checkpoints will be saved to:\", OUT_DIR)\n","\n"]},{"cell_type":"markdown","id":"96ae104c-ae6c-4e9a-9de8-c2aabcad7c12","metadata":{"id":"96ae104c-ae6c-4e9a-9de8-c2aabcad7c12"},"source":["This cell defines how our model training will run. We tell Hugging Face where to save the model and logs, how often to evaluate and save checkpoints (once per epoch), and which metric to use to keep the best version (accuracy). We also set basic hyperparameters like number of epochs, batch size, learning rate, and how often to show progress. At the end, the print confirms that evaluation and saving are synchronized (both every 801 steps), which means the training is correctly configured.\n"]},{"cell_type":"markdown","id":"e2845090-243b-4911-8cbe-421b0372cc22","metadata":{"id":"e2845090-243b-4911-8cbe-421b0372cc22"},"source":["**Goal**: create the Trainer object that connects the model, data, and training configuration."]},{"cell_type":"code","execution_count":null,"id":"09f38ce3-8b2a-4088-9887-2195b2155bff","metadata":{"id":"09f38ce3-8b2a-4088-9887-2195b2155bff","outputId":"e178220d-9f8f-4a7b-e1d2-811421e2537f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755937233902,"user_tz":-120,"elapsed":130,"user":{"displayName":"Patricia Gim√©nez","userId":"16444508660089000781"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Trainer created successfully\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1971883890.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]}],"source":["# Metrics & Trainer\n","\n","# Accuracy metric\n","def compute_metrics(eval_pred):\n","    try:\n","        from transformers import EvalPrediction\n","        if isinstance(eval_pred, EvalPrediction):\n","            logits, labels = eval_pred.predictions, eval_pred.label_ids\n","        else:\n","            logits, labels = eval_pred\n","    except Exception:\n","        logits, labels = eval_pred\n","\n","    if isinstance(logits, (tuple, list)):\n","        logits = logits[0]\n","\n","    preds = np.argmax(logits, axis=-1)\n","    acc = (preds == labels).mean()\n","    return {\"accuracy\": float(acc)}\n","\n","# Create the Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_ds,\n","    eval_dataset=val_ds,\n","    processing_class=processor,\n","    compute_metrics=compute_metrics,\n","    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",")\n","\n","print(\"Trainer created successfully\")\n"]},{"cell_type":"code","execution_count":null,"id":"0ef1f2b5-7a6a-45fa-aaa1-244752fce2f1","metadata":{"id":"0ef1f2b5-7a6a-45fa-aaa1-244752fce2f1","colab":{"base_uri":"https://localhost:8080/","height":296},"executionInfo":{"status":"ok","timestamp":1755938389389,"user_tz":-120,"elapsed":112865,"user":{"displayName":"Patricia Gim√©nez","userId":"16444508660089000781"}},"outputId":"e2c1ac87-c7de-4907-a35b-6e7235ec6c18"},"outputs":[{"output_type":"stream","name":"stdout","text":["No checkpoint found ‚Üí training from scratch.\n","üöÄ Starting training...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='603' max='603' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [603/603 01:42, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.106400</td>\n","      <td>0.061728</td>\n","      <td>0.983151</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.037700</td>\n","      <td>0.040930</td>\n","      <td>0.990016</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.015300</td>\n","      <td>0.037245</td>\n","      <td>0.988768</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" Training finished in 1.8 min (0.03 h)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [51/51 00:05]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["üìä Validation metrics: {'eval_loss': 0.04093022644519806, 'eval_accuracy': 0.9900156006240249, 'eval_runtime': 6.0218, 'eval_samples_per_second': 532.235, 'eval_steps_per_second': 8.469, 'epoch': 3.0}\n"," Saved to: /content/drive/MyDrive/ecoscan/models/vit_ecoscan_v1\n"]}],"source":["# ---- SAFE RESUME TRAINING BLOCK ----\n","import os, time, json, pathlib, torch\n","from transformers.trainer_utils import get_last_checkpoint\n","\n","torch.backends.cudnn.benchmark = True  # peque√±o speed-up en GPU\n","\n","# Asegura que el directorio de salida existe\n","pathlib.Path(training_args.output_dir).mkdir(parents=True, exist_ok=True)\n","\n","# Detecta √∫ltimo checkpoint (si existe)\n","last_ckpt = get_last_checkpoint(training_args.output_dir)\n","if last_ckpt is not None:\n","    print(\" Found checkpoint ‚Üí resuming from:\", last_ckpt)\n","else:\n","    print(\"No checkpoint found ‚Üí training from scratch.\")\n","resume_arg = last_ckpt  # None si no hay checkpoint\n","\n","print(\"üöÄ Starting training...\")\n","t0 = time.time()\n","train_result = trainer.train(resume_from_checkpoint=resume_arg)\n","t_secs = time.time() - t0\n","print(f\" Training finished in {t_secs/60:.1f} min ({t_secs/3600:.2f} h)\")\n","\n","# ---- SAVE MODEL + PROCESSOR + TRAINER STATE ----\n","OUT_DIR = training_args.output_dir  # usamos el mismo output_dir\n","trainer.save_model(OUT_DIR)          # model + config\n","processor.save_pretrained(OUT_DIR)   # preprocessor para inferencia\n","trainer.save_state()                 # estado del optimizador, etc.\n","\n","# ---- EVALUATION ----\n","eval_metrics = trainer.evaluate()\n","print(\"üìä Validation metrics:\", eval_metrics)\n","\n","# ---- SUMMARY JSON ----\n","summary = {\n","    \"runtime_sec\": float(t_secs),\n","    \"runtime_min\": float(t_secs / 60),\n","    \"runtime_hours\": float(t_secs / 3600),\n","    \"epochs\": float(training_args.num_train_epochs),\n","    \"batch_size\": int(training_args.per_device_train_batch_size),\n","    \"train_images\": len(train_ds),\n","    \"val_images\": len(val_ds),\n","    **{k: float(v) for k, v in eval_metrics.items()},\n","}\n","with open(os.path.join(OUT_DIR, \"run_summary.json\"), \"w\") as f:\n","    json.dump(summary, f, indent=2)\n","\n","print(\" Saved to:\", OUT_DIR)\n","\n"]},{"cell_type":"markdown","source":["## Training Results Summary\n","\n","- The Vision Transformer (ViT) was fine-tuned successfully on the **garbage classification dataset** (15 classes, ~16k images).  \n","- **Training completed in ~1.8 minutes** on an NVIDIA A100 GPU.  \n","- **Final performance:**\n","  - Training Loss: **0.015**\n","  - Validation Loss: **0.037**\n","  - Validation Accuracy: **99.0%**\n","\n","###  Interpretation\n","- The model shows **very high accuracy** (‚âà99%) on the validation set, indicating it learned to distinguish the waste categories effectively.  \n","- The **training and validation loss decreased smoothly**, suggesting good convergence and **no severe overfitting**.  \n","- These results demonstrate that the ViT is a strong choice for this dataset.  \n","- Next steps should include:\n","  - **Per-class evaluation** (precision, recall, F1) to check whether smaller classes (e.g., *electronics*, *oil*) are as well recognized as dominant ones (e.g., *clothes*).  \n","  - **Confusion matrix** inspection to detect potential misclassifications between visually similar categories (e.g., *glass vs brown-glass*).  \n","  - Exporting the model and testing it in a **real-world app scenario** (uploading an image and predicting the category).  \n","\n","Overall, the model is ready for deployment and further evaluation in production-like settings.\n"],"metadata":{"id":"G45JGyMuF6si"},"id":"G45JGyMuF6si"}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"A100"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"2b7792def8fb464fa8b579a11fa511dc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_361144578f3243a5895be2b95e464105","IPY_MODEL_b45b3fd6534f4770a40b1409f67e0344","IPY_MODEL_866eabe4f0b7454daf43a955bfa3695f"],"layout":"IPY_MODEL_73add9c8095c4f7e89fbd68c5ab93aea"}},"361144578f3243a5895be2b95e464105":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f49f69730ef45a982c12237657054f8","placeholder":"‚Äã","style":"IPY_MODEL_4acb64e89bca42ed8a47d46b23214178","value":"preprocessor_config.json:‚Äá100%"}},"b45b3fd6534f4770a40b1409f67e0344":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_db95e9c7c0494599a6db90fcd9e857ce","max":160,"min":0,"orientation":"horizontal","style":"IPY_MODEL_13e83674f40e489b99cfa08f907f4777","value":160}},"866eabe4f0b7454daf43a955bfa3695f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_907de738b82a489baeaf633a3d6bb42a","placeholder":"‚Äã","style":"IPY_MODEL_12bf0c1b128941ccaafbf427840bf00f","value":"‚Äá160/160‚Äá[00:00&lt;00:00,‚Äá18.2kB/s]"}},"73add9c8095c4f7e89fbd68c5ab93aea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f49f69730ef45a982c12237657054f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4acb64e89bca42ed8a47d46b23214178":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db95e9c7c0494599a6db90fcd9e857ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13e83674f40e489b99cfa08f907f4777":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"907de738b82a489baeaf633a3d6bb42a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12bf0c1b128941ccaafbf427840bf00f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"37579880a8c14a499704231e773693e7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f14edd69efae4876ae91ca146e5b875e","IPY_MODEL_f16532bfaa12486cb35d3ac852b1d6c7","IPY_MODEL_5b1a17b7217a4643944b24d0870aaa17"],"layout":"IPY_MODEL_295e7b7b90844873b0202d9a37cc1a44"}},"f14edd69efae4876ae91ca146e5b875e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3e42ea8b6be4f3caabd52cca49885f6","placeholder":"‚Äã","style":"IPY_MODEL_46da212eed214e6ab229022ba69628f0","value":"config.json:‚Äá"}},"f16532bfaa12486cb35d3ac852b1d6c7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9eaa19474e5424d875b117a0881e105","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bb1a3b4a5c5849f89e426cae7614cb1e","value":1}},"5b1a17b7217a4643944b24d0870aaa17":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8cdc0778c427454491cd45eeb151c831","placeholder":"‚Äã","style":"IPY_MODEL_5ad5757dce4e45b6b78cf7cf5d17ca9f","value":"‚Äá69.7k/?‚Äá[00:00&lt;00:00,‚Äá7.16MB/s]"}},"295e7b7b90844873b0202d9a37cc1a44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3e42ea8b6be4f3caabd52cca49885f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46da212eed214e6ab229022ba69628f0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f9eaa19474e5424d875b117a0881e105":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"bb1a3b4a5c5849f89e426cae7614cb1e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8cdc0778c427454491cd45eeb151c831":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ad5757dce4e45b6b78cf7cf5d17ca9f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"efc9d02993d04506a65b5c5d66c8876a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_18912818b88a4cd8af79384056a22407","IPY_MODEL_22d916c4513e4335a7152171906e1b67","IPY_MODEL_1fc270f5a9f446caacb2b2ee4618dc31"],"layout":"IPY_MODEL_638d8bd65e0e4e98a9b68f0563a1fd8e"}},"18912818b88a4cd8af79384056a22407":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e3feeb8377e400bbb38cbb6259737c0","placeholder":"‚Äã","style":"IPY_MODEL_1cb99b7ce37a4cb5a912f574406180f6","value":"model.safetensors:‚Äá100%"}},"22d916c4513e4335a7152171906e1b67":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6a6bfff2ed24292b00d95a54f8ff846","max":346293852,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f20ea386a59b4300ae4233611dd55030","value":346293852}},"1fc270f5a9f446caacb2b2ee4618dc31":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5073a28500dc4a0f852d30f3f58b96d9","placeholder":"‚Äã","style":"IPY_MODEL_94d2041e8f9645d18eebb6d8cd5355dc","value":"‚Äá346M/346M‚Äá[00:01&lt;00:00,‚Äá295MB/s]"}},"638d8bd65e0e4e98a9b68f0563a1fd8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e3feeb8377e400bbb38cbb6259737c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1cb99b7ce37a4cb5a912f574406180f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c6a6bfff2ed24292b00d95a54f8ff846":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f20ea386a59b4300ae4233611dd55030":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5073a28500dc4a0f852d30f3f58b96d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94d2041e8f9645d18eebb6d8cd5355dc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}