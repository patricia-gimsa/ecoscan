{"cells":[{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","from pathlib import Path\n","import os\n","\n","\n","# Mount Drive to access dataset and model folders\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A5Dpuuu4oUYq","executionInfo":{"status":"ok","timestamp":1755982719291,"user_tz":-120,"elapsed":22885,"user":{"displayName":"Patricia Giménez","userId":"16444508660089000781"}},"outputId":"e6cd3e0f-2e8c-44ac-c83f-b464dfc29003"},"id":"A5Dpuuu4oUYq","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["#  Setup: imports + model/processor + device\n","\n","import os, json\n","from pathlib import Path\n","import numpy as np\n","from PIL import Image, ImageEnhance\n","import torch\n","from transformers import AutoImageProcessor, AutoModelForImageClassification\n","\n","# your model folder\n","MODEL_DIR = Path(\"/content/drive/MyDrive/ecoscan/models/vit_ecoscan_v1\")\n","\n","# Pick device (GPU if available)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","# Load processor + model (make sure these match the model you trained)\n","processor = AutoImageProcessor.from_pretrained(MODEL_DIR)\n","model = AutoModelForImageClassification.from_pretrained(MODEL_DIR).to(device)\n","model.eval()\n","\n","print(\"num_labels:\", model.config.num_labels)\n","\n","# show human-readable class names if available\n","id2label = getattr(model.config, \"id2label\", None)\n","class_names = [id2label.get(str(i), id2label.get(i, str(i))) for i in range(model.config.num_labels)] if id2label else None\n","print(\"class_names:\", class_names if class_names else \"(no id2label found, will show numeric IDs)\")\n","\n"],"metadata":{"id":"x6E02TJJokBX","executionInfo":{"status":"ok","timestamp":1755987904455,"user_tz":-120,"elapsed":404,"user":{"displayName":"Patricia Giménez","userId":"16444508660089000781"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c75798eb-b54f-4aa0-f405-9dcbd29a23b5"},"id":"x6E02TJJokBX","execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","num_labels: 15\n","class_names: ['battery', 'brown-glass', 'cardboard', 'clothes', 'electronics', 'green-glass', 'metal_packaging', 'oil', 'organic', 'paper', 'plastic', 'shoes', 'tetrapak', 'trash', 'white-glass']\n"]}]},{"cell_type":"markdown","source":["The model and processor loaded correctly and we’re using the GPU (cuda) for fast inference. The printout confirms a ViT with 12 transformer layers, hidden size 768, patch size 16×16, and a classifier head with 15 outputs (your 15 recycling classes). The model is in eval() mode, so we’re ready to run real-photo tests in the next cells."],"metadata":{"id":"HR8-_rs_vGXN"},"id":"HR8-_rs_vGXN"},{"cell_type":"code","source":["# Load the saved Energy threshold (from 04)\n","# We will use Energy (T=1.0) and the chosen threshold to accept/reject\n","\n","EVAL_DIR = MODEL_DIR / \"eval\"\n","with open(EVAL_DIR / \"reject_threshold.json\") as f:\n","    cfg = json.load(f)\n","\n","thr = float(cfg[\"reject_threshold\"])\n","print(\"Loaded threshold:\", thr)\n","print(\"Config keys:\", list(cfg.keys()))\n","print(\"Rule:\", cfg.get(\"rule\", \"accept if energy <= threshold; else reject\"))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dTVQ-VUAvOjJ","executionInfo":{"status":"ok","timestamp":1755987909398,"user_tz":-120,"elapsed":1562,"user":{"displayName":"Patricia Giménez","userId":"16444508660089000781"}},"outputId":"3f3bbbe7-793d-4a18-87f1-cf9dec6f684f"},"id":"dTVQ-VUAvOjJ","execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded threshold: -4.222049236297607\n","Config keys: ['method', 'reject_threshold', 'target_ood_reject', 'rule', 'metrics_at_threshold']\n","Rule: accept if energy <= threshold; else reject\n"]}]},{"cell_type":"markdown","source":["We successfully loaded the saved operating point: Energy (T=1.0) with threshold ≈ −5.4664.\n","Rule to use from now on: accept if energy ≤ threshold; reject otherwise.\n","This is the 85% OOD-rejection setting we chose, so every new photo will be judged by this simple rule."],"metadata":{"id":"eGiVwRQMvf1R"},"id":"eGiVwRQMvf1R"},{"cell_type":"code","source":["# Core helper: compute Energy + accept/reject\n","\n","@torch.no_grad()\n","def logits_from_image(img: Image.Image):\n","    \"\"\"Get logits from the model for a single PIL image.\"\"\"\n","    enc = processor(images=img.convert(\"RGB\"), return_tensors=\"pt\").to(device)\n","    out = model(**enc)\n","    return out.logits.detach().cpu().numpy()[0]  # shape: (num_classes,)\n","\n","def energy_from_logits_np(logits: np.ndarray, T: float = 1.0) -> float:\n","    \"\"\"\n","    Energy score (T=1.0 by default).\n","    Lower energy = more likely in-domain.\n","    \"\"\"\n","    x = logits / T\n","    m = np.max(x)\n","    lse = m + np.log(np.exp(x - m).sum())\n","    return float(-T * lse)\n","\n","def predict_one_with_reject(img_path: str, class_names=None):\n","    \"\"\"\n","    Returns a dict with: pred_class_id, pred_class_name, max_softmax, energy, rejected\n","    \"\"\"\n","    img = Image.open(img_path).convert(\"RGB\")\n","    logits = logits_from_image(img)\n","    probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n","    pred_id = int(np.argmax(probs))\n","    pred_name = class_names[pred_id] if class_names is not None else str(pred_id)\n","    energy = energy_from_logits_np(logits, T=1.0)\n","    rejected = energy > thr  # IMPORTANT: accept if energy <= thr, reject otherwise\n","    return {\n","        \"path\": img_path,\n","        \"pred_class_id\": pred_id,\n","        \"pred_class_name\": pred_name,\n","        \"max_softmax\": float(probs[pred_id]),\n","        \"energy\": energy,\n","        \"threshold\": thr,\n","        \"rejected\": bool(rejected),\n","    }\n"],"metadata":{"id":"xPZGI68TviQc","executionInfo":{"status":"ok","timestamp":1755987920560,"user_tz":-120,"elapsed":7,"user":{"displayName":"Patricia Giménez","userId":"16444508660089000781"}}},"id":"xPZGI68TviQc","execution_count":16,"outputs":[]},{"cell_type":"code","source":["# Load real photos from Drive (ID: subfolders per class, OOD: one folder)\n","\n","from pathlib import Path\n","\n","PHOTOS_ROOT = Path(\"/content/drive/MyDrive/ecoscan/data/my_photos\")\n","ID_DIR  = PHOTOS_ROOT / \"id\"   # in-domain photos -> subfolders named exactly like your classes\n","OOD_DIR = PHOTOS_ROOT / \"ood\"  # out-of-domain photos\n","\n","IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\"}\n","\n","def gather_id_samples(id_root, valid_class_names=None):\n","    \"\"\"Return list of (path, expected_class_name). Expects subfolders per class.\"\"\"\n","    items = []\n","    if not id_root.exists():\n","        print(f\"Missing folder: {id_root}\")\n","        return items\n","\n","    # build a case-insensitive map to the model's canonical class names (if available)\n","    name_map = None\n","    if valid_class_names:\n","        name_map = {str(n).lower(): str(n) for n in valid_class_names}\n","\n","    for cls_dir in sorted(p for p in id_root.iterdir() if p.is_dir()):\n","        folder_name = cls_dir.name\n","        expected = folder_name\n","        if name_map:\n","            expected = name_map.get(folder_name.lower(), folder_name)  # tolerate case differences\n","            if expected not in valid_class_names:\n","                print(f\" Folder '{folder_name}' not recognized in model classes. Using as-is.\")\n","        for p in sorted(cls_dir.iterdir()):\n","            if p.is_file() and p.suffix.lower() in IMG_EXTS:\n","                items.append((str(p), expected))\n","    return items\n","\n","def gather_paths(folder):\n","    if not folder.exists():\n","        print(f\" Missing folder: {folder}\")\n","        return []\n","    return [str(p) for p in sorted(folder.iterdir())\n","            if p.is_file() and p.suffix.lower() in IMG_EXTS]\n","\n","# Use class_names from your Celda 2 (may be None, that’s fine)\n","ID_SAMPLES  = gather_id_samples(ID_DIR, valid_class_names=class_names)\n","OOD_SAMPLES = gather_paths(OOD_DIR)\n","\n","print(f\"Found {len(ID_SAMPLES)} in-domain photos with expected class.\")\n","print(f\"Found {len(OOD_SAMPLES)} OOD photos.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pgU8fmRu4Gwg","executionInfo":{"status":"ok","timestamp":1755987973201,"user_tz":-120,"elapsed":51,"user":{"displayName":"Patricia Giménez","userId":"16444508660089000781"}},"outputId":"13d90b08-6fc0-4bea-a505-dcf75d6c205a"},"id":"pgU8fmRu4Gwg","execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 3 in-domain photos with expected class.\n","Found 4 OOD photos.\n"]}]},{"cell_type":"code","source":["# Run inference on all photos and print a per-image table\n","\n","from pathlib import Path\n","\n","results = []\n","\n","# In-domain with expected class\n","for path, expected in ID_SAMPLES:\n","    r = predict_one_with_reject(path, class_names=class_names)\n","    r[\"expected_type\"] = \"ID\"\n","    r[\"expected_class_name\"] = expected\n","    r[\"decision\"] = \"ACCEPT\" if not r[\"rejected\"] else \"REJECT\"\n","    # outcomes\n","    r[\"is_correct_class\"] = (not r[\"rejected\"]) and (r[\"pred_class_name\"] == expected)\n","    r[\"is_wrong_class\"]   = (not r[\"rejected\"]) and (r[\"pred_class_name\"] != expected)\n","    r[\"is_false_reject\"]  = r[\"rejected\"]\n","    results.append(r)\n","\n","# OOD (no expected class)\n","for path in OOD_SAMPLES:\n","    r = predict_one_with_reject(path, class_names=class_names)\n","    r[\"expected_type\"] = \"OOD\"\n","    r[\"expected_class_name\"] = None\n","    r[\"decision\"] = \"ACCEPT\" if not r[\"rejected\"] else \"REJECT\"\n","    r[\"is_correct_ood_reject\"] = r[\"rejected\"]\n","    r[\"is_false_accept\"]       = not r[\"rejected\"]\n","    results.append(r)\n","\n","# Pretty print per image\n","print(\"file | expected_type | expected_class | pred | prob | energy | thr | decision | note\")\n","for r in results:\n","    fname = Path(r[\"path\"]).name\n","    if r[\"expected_type\"] == \"ID\":\n","        note = \"correct\" if r[\"is_correct_class\"] else (\"wrong-class\" if r[\"is_wrong_class\"] else \"false-reject\")\n","        print(f\"{fname} | ID | {r['expected_class_name']} | {r['pred_class_name']} | \"\n","              f\"{r['max_softmax']:.3f} | {r['energy']:.3f} | {r['threshold']:.3f} | {r['decision']} | {note}\")\n","    else:\n","        note = \"rejected\" if r[\"is_correct_ood_reject\"] else \"false-accept\"\n","        print(f\"{fname} | OOD | - | {r['pred_class_name']} | \"\n","              f\"{r['max_softmax']:.3f} | {r['energy']:.3f} | {r['threshold']:.3f} | {r['decision']} | {note}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NtpjfXrK9K7e","executionInfo":{"status":"ok","timestamp":1755987976828,"user_tz":-120,"elapsed":1606,"user":{"displayName":"Patricia Giménez","userId":"16444508660089000781"}},"outputId":"73e914f0-1e56-4690-bf5f-9b94a9bdd75c"},"id":"NtpjfXrK9K7e","execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["file | expected_type | expected_class | pred | prob | energy | thr | decision | note\n","detergent_box_01.JPG | ID | cardboard | paper | 0.929 | -6.512 | -4.222 | ACCEPT | wrong-class\n","crushed_can_01.JPG | ID | metal_packaging | metal_packaging | 0.632 | -4.336 | -4.222 | ACCEPT | correct\n","crushed_plastic_bottle_01.JPG | ID | plastic | plastic | 0.981 | -6.945 | -4.222 | ACCEPT | correct\n","cat_01.JPG | OOD | - | clothes | 0.241 | -3.190 | -4.222 | REJECT | rejected\n","coffe_pod_01.JPG | OOD | - | metal_packaging | 0.348 | -3.444 | -4.222 | REJECT | rejected\n","plant_01.JPG | OOD | - | cardboard | 0.461 | -3.999 | -4.222 | REJECT | rejected\n","thermomix_01.JPG | OOD | - | plastic | 0.414 | -3.544 | -4.222 | REJECT | rejected\n"]}]},{"cell_type":"markdown","source":["Using the Energy threshold ≈ −4.22, the demo works as intended:\n","\n","In-domain (3 photos): all ACCEPTED.\n","\n","Plastic bottle: correct.\n","\n","Crushed can: correct (now passes the gate).\n","\n","Detergent box: ACCEPTED but wrong class (paper vs expected cardboard).\n","\n","OOD (4 photos): all REJECTED (cat, coffee pod, plant, thermomix).\n","\n","So the reject-unknowns gate is solid, and class predictions are mostly right on your real photos, with one reasonable confusion (paper vs cardboard)."],"metadata":{"id":"Xtzbpb44CxLJ"},"id":"Xtzbpb44CxLJ"},{"cell_type":"code","source":["# Summary counts for the demo\n","id_total = sum(1 for r in results if r[\"expected_type\"]==\"ID\")\n","id_correct = sum(1 for r in results if r[\"expected_type\"]==\"ID\" and r[\"is_correct_class\"])\n","id_wrong   = sum(1 for r in results if r[\"expected_type\"]==\"ID\" and r[\"is_wrong_class\"])\n","id_rej     = sum(1 for r in results if r[\"expected_type\"]==\"ID\" and r[\"is_false_reject\"])\n","\n","ood_total = sum(1 for r in results if r[\"expected_type\"]==\"OOD\")\n","ood_rej   = sum(1 for r in results if r[\"expected_type\"]==\"OOD\" and r[\"is_correct_ood_reject\"])\n","ood_acc   = sum(1 for r in results if r[\"expected_type\"]==\"OOD\" and r[\"is_false_accept\"])\n","\n","print(\"== Demo summary ==\")\n","print(f\"ID accepted & correct: {id_correct}/{id_total} ({(id_correct/max(1,id_total))*100:.1f}%)\")\n","print(f\"ID accepted but wrong-class: {id_wrong}/{id_total} ({(id_wrong/max(1,id_total))*100:.1f}%)\")\n","print(f\"ID rejected (false-reject): {id_rej}/{id_total} ({(id_rej/max(1,id_total))*100:.1f}%)\")\n","print(f\"OOD rejected: {ood_rej}/{ood_total} ({(ood_rej/max(1,ood_total))*100:.1f}%)\")\n","print(f\"OOD accepted (false-accept): {ood_acc}/{ood_total} ({(ood_acc/max(1,ood_total))*100:.1f}%)\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"myCaaG2yDked","executionInfo":{"status":"ok","timestamp":1755988255594,"user_tz":-120,"elapsed":5,"user":{"displayName":"Patricia Giménez","userId":"16444508660089000781"}},"outputId":"727b6b1d-e017-4209-9f57-440ed4e735ef"},"id":"myCaaG2yDked","execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["== Demo summary ==\n","ID accepted & correct: 2/3 (66.7%)\n","ID accepted but wrong-class: 1/3 (33.3%)\n","ID rejected (false-reject): 0/3 (0.0%)\n","OOD rejected: 4/4 (100.0%)\n","OOD accepted (false-accept): 0/4 (0.0%)\n"]}]},{"cell_type":"code","source":["# Robustness check (fixed): brightness & rotation using PIL images directly\n","from PIL import Image, ImageEnhance\n","from pathlib import Path\n","\n","TEST_ID  = ID_SAMPLES[0][0] if ID_SAMPLES else None\n","TEST_OOD = OOD_SAMPLES[0]    if OOD_SAMPLES else None\n","\n","def check(img_path, tag):\n","    if not img_path or not Path(img_path).exists():\n","        print(f\"No valid {tag} image path.\");\n","        return\n","\n","    base = Image.open(img_path).convert(\"RGB\")\n","\n","    def infer_from_pil(img_obj):\n","        # Use the helper that accepts PIL images\n","        logits = logits_from_image(img_obj)\n","        probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n","        pred_id = int(np.argmax(probs))\n","        pred_name = class_names[pred_id] if class_names is not None else str(pred_id)\n","        energy = energy_from_logits_np(logits, T=1.0)\n","        rejected = energy > thr  # accept if energy <= thr\n","        return pred_name, float(probs[pred_id]), float(energy), bool(rejected)\n","\n","    print(f\"\\n== {tag} ==\")\n","    variants = [\n","        (\"base\", base),\n","        (\"brightness x0.7\", ImageEnhance.Brightness(base).enhance(0.7)),\n","        (\"brightness x1.3\", ImageEnhance.Brightness(base).enhance(1.3)),\n","        (\"+15°\", base.rotate(15, expand=True)),\n","        (\"-15°\", base.rotate(-15, expand=True)),\n","    ]\n","    for name, img in variants:\n","        pred, prob, energy, rejected = infer_from_pil(img)\n","        print(f\"{Path(img_path).name} [{name}] -> pred={pred}, \"\n","              f\"prob={prob:.3f}, energy={energy:.3f}, thr={thr:.3f}, rejected={rejected}\")\n","\n","check(TEST_ID, \"ID perturbations\")\n","check(TEST_OOD, \"OOD perturbations\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NWepbCI6CoVN","executionInfo":{"status":"ok","timestamp":1755988428175,"user_tz":-120,"elapsed":2765,"user":{"displayName":"Patricia Giménez","userId":"16444508660089000781"}},"outputId":"807fff15-cc5c-44c8-a903-6c01be73cab1"},"id":"NWepbCI6CoVN","execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","== ID perturbations ==\n","detergent_box_01.JPG [base] -> pred=paper, prob=0.929, energy=-6.512, thr=-4.222, rejected=False\n","detergent_box_01.JPG [brightness x0.7] -> pred=paper, prob=0.835, energy=-6.090, thr=-4.222, rejected=False\n","detergent_box_01.JPG [brightness x1.3] -> pred=paper, prob=0.946, energy=-6.851, thr=-4.222, rejected=False\n","detergent_box_01.JPG [+15°] -> pred=paper, prob=0.971, energy=-6.937, thr=-4.222, rejected=False\n","detergent_box_01.JPG [-15°] -> pred=paper, prob=0.988, energy=-7.597, thr=-4.222, rejected=False\n","\n","== OOD perturbations ==\n","cat_01.JPG [base] -> pred=clothes, prob=0.241, energy=-3.190, thr=-4.222, rejected=True\n","cat_01.JPG [brightness x0.7] -> pred=clothes, prob=0.386, energy=-3.307, thr=-4.222, rejected=True\n","cat_01.JPG [brightness x1.3] -> pred=shoes, prob=0.165, energy=-3.180, thr=-4.222, rejected=True\n","cat_01.JPG [+15°] -> pred=cardboard, prob=0.467, energy=-3.440, thr=-4.222, rejected=True\n","cat_01.JPG [-15°] -> pred=cardboard, prob=0.646, energy=-3.784, thr=-4.222, rejected=True\n"]}]},{"cell_type":"markdown","source":["The accept/reject gate is robust to small lighting/angle changes; one class confusion (paper vs cardboard) persists, which is fine to mention as a known limitation."],"metadata":{"id":"IpPe8gqbEmZ4"},"id":"IpPe8gqbEmZ4"},{"cell_type":"code","source":["# Finalize: save demo results to Drive (JSON + CSV)\n","from pathlib import Path\n","import json, csv\n","\n","OUT = MODEL_DIR / \"eval\"\n","OUT.mkdir(parents=True, exist_ok=True)\n","\n","# JSON\n","with open(OUT / \"inference_demo_results.json\", \"w\") as f:\n","    json.dump(results, f, indent=2)\n","\n","# CSV\n","def note_for(r):\n","    if r[\"expected_type\"] == \"ID\":\n","        return \"correct\" if r[\"is_correct_class\"] else (\"wrong-class\" if r[\"is_wrong_class\"] else \"false-reject\")\n","    else:\n","        return \"rejected\" if r[\"is_correct_ood_reject\"] else \"false-accept\"\n","\n","csv_path = OUT / \"inference_demo_results.csv\"\n","with open(csv_path, \"w\", newline=\"\") as f:\n","    w = csv.writer(f)\n","    w.writerow([\"path\",\"expected_type\",\"expected_class\",\"pred\",\"prob\",\"energy\",\"threshold\",\"decision\",\"rejected\",\"note\"])\n","    for r in results:\n","        w.writerow([\n","            r[\"path\"], r[\"expected_type\"], r.get(\"expected_class_name\"),\n","            r[\"pred_class_name\"], f\"{r['max_softmax']:.5f}\",\n","            f\"{r['energy']:.5f}\", f\"{r['threshold']:.5f}\",\n","            r[\"decision\"], r[\"rejected\"], note_for(r)\n","        ])\n","\n","print(\"Saved:\", OUT / \"inference_demo_results.json\")\n","print(\"Saved:\", OUT / \"inference_demo_results.csv\")\n","print(\"DONE: 05 notebook ready ✅\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7pytpRWkFYBW","executionInfo":{"status":"ok","timestamp":1755988728511,"user_tz":-120,"elapsed":23,"user":{"displayName":"Patricia Giménez","userId":"16444508660089000781"}},"outputId":"1954a8d9-9c31-4423-fa04-67c24be2dd51"},"id":"7pytpRWkFYBW","execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved: /content/drive/MyDrive/ecoscan/models/vit_ecoscan_v1/eval/inference_demo_results.json\n","Saved: /content/drive/MyDrive/ecoscan/models/vit_ecoscan_v1/eval/inference_demo_results.csv\n","DONE: 05 notebook ready ✅\n"]}]},{"cell_type":"markdown","source":["Demo summary. Using the saved Energy threshold (≈ −4.22, 75% OOD target), all in-domain photos were accepted (two with the correct class; one reasonable paper vs cardboard confusion) and all OOD photos were rejected. We also tried small lighting/rotation changes: decisions stayed stable. We saved the per-image results to eval/inference_demo_results.json and .csv for reporting and reproducibility."],"metadata":{"id":"AzCvpBJ8FdJO"},"id":"AzCvpBJ8FdJO"}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}