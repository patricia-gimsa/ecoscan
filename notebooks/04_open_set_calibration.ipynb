{"cells":[{"cell_type":"code","source":["# 04_open_set_calibration.ipynb\n","# This notebook generates 200 out-of-distribution (OOD) images and calculates a rejection threshold\n","# for detecting unknown inputs in your Vision Transformer model.\n","\n","\n","# Mount Google Drive\n","from google.colab import drive\n","from pathlib import Path\n","import os\n","\n","\n","# Mount Drive to access dataset and model folders\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A5Dpuuu4oUYq","executionInfo":{"status":"ok","timestamp":1755987280498,"user_tz":-120,"elapsed":32886,"user":{"displayName":"Patricia Giménez","userId":"16444508660089000781"}},"outputId":"0959e51b-f53c-47b8-baa3-ae2f390b2335"},"id":"A5Dpuuu4oUYq","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Install a robust image crawler\n","!pip -q install icrawler pillow tqdm"],"metadata":{"id":"x6E02TJJokBX"},"id":"x6E02TJJokBX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define path to your OOD samples directory (already created manually)\n","OOD_DIR = Path(\"/content/drive/MyDrive/ecoscan/data/ood_samples\")\n","OOD_DIR.mkdir(parents=True, exist_ok=True)\n","print(\"Saving OOD images to:\", OOD_DIR)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O_-JRPpNog99","executionInfo":{"status":"ok","timestamp":1755987291599,"user_tz":-120,"elapsed":1842,"user":{"displayName":"Patricia Giménez","userId":"16444508660089000781"}},"outputId":"5729bff2-e149-436b-9f62-5fd1a011cfe7"},"id":"O_-JRPpNog99","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Saving OOD images to: /content/drive/MyDrive/ecoscan/data/ood_samples\n"]}]},{"cell_type":"code","source":["# A temporary local folder to download first (faster), then we'll deduplicate and move to Drive\n","TMP_DIR = Path(\"/content/ood_tmp\")\n","TMP_DIR.mkdir(parents=True, exist_ok=True)"],"metadata":{"id":"UEy8kl9PqmDC"},"id":"UEy8kl9PqmDC","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os, io, time, hashlib, shutil, random\n","from pathlib import Path\n","from PIL import Image\n","from tqdm import tqdm\n","from icrawler.builtin import BingImageCrawler\n","\n","# Keywords unrelated to your 15 classes\n","KEYWORDS = [\n","    \"golden retriever dog\",\"tabby cat portrait\",\"parrot bird colorful\",\"wild horse running\",\n","    \"sports car on road\",\"motorcycle racing\",\"airplane cockpit\",\"sailboat at sea\",\n","    \"mountain landscape\",\"tropical beach\",\"desert dunes\",\"rainforest canopy\",\n","    \"wooden chair\",\"leather couch\",\"keyboard close-up\",\"gaming laptop\",\n","    \"coffee mug on desk\",\"wrist watch macro\",\"guitar electric\",\"violin instrument\",\n","    \"sushi platter\",\"pizza slice\",\"pancakes breakfast\",\"fruit basket assorted\",\n","    \"basketball game\",\"yoga pose outdoors\",\"runner in stadium\",\"skateboard trick\",\n","    \"modern skyscraper\",\"old castle\",\"city skyline at night\",\"bridge over river\",\n","    \"abstract painting\",\"graffiti wall\",\"marble texture\",\"neon lights\",\n","    \"drone flying\",\"smartphone on table\",\"camera lens macro\",\"headphones studio\",\n","    \"library bookshelves\",\"kitchen interior\",\"bedroom minimal\",\"office workspace\",\n","    \"snowman winter\",\"hot air balloon\",\"camping tent forest\",\"fireworks festival\",\n","]\n","\n","TARGET_TOTAL = 200\n","MAX_PER_KEYWORD = 6         # small per keyword → fewer 403s\n","OVERFETCH = MAX_PER_KEYWORD * 3\n","MIN_SIDE = 128              # reject tiny thumbs\n","\n","def sha1_file(p: Path) -> str:\n","    h = hashlib.sha1()\n","    with p.open(\"rb\") as f:\n","        for chunk in iter(lambda: f.read(8192), b\"\"):\n","            h.update(chunk)\n","    return h.hexdigest()\n","\n","# Existing hashes in Drive (resume-safe)\n","existing_hashes = set()\n","for p in OOD_DIR.glob(\"*\"):\n","    if p.is_file():\n","        try:\n","            existing_hashes.add(sha1_file(p))\n","        except Exception:\n","            pass\n","\n","saved = len(list(OOD_DIR.glob(\"*\")))\n","print(f\"Already present in OOD folder: {saved}\")\n","\n","for q in tqdm(KEYWORDS, desc=\"Downloading OOD\"):\n","    if saved >= TARGET_TOTAL:\n","        break\n","\n","    # clean tmp subfolder\n","    kw_dir = TMP_DIR / q.replace(\" \", \"_\")[:40]\n","    if kw_dir.exists():\n","        shutil.rmtree(kw_dir, ignore_errors=True)\n","    kw_dir.mkdir(parents=True, exist_ok=True)\n","\n","    # Polite crawler: low threads, overfetch, min_size filter\n","    crawler = BingImageCrawler(\n","        feeder_threads=1,\n","        parser_threads=1,\n","        downloader_threads=1,\n","        storage={\"root_dir\": str(kw_dir)},\n","    )\n","    try:\n","        crawler.crawl(\n","            keyword=q,\n","            max_num=OVERFETCH,\n","            min_size=(MIN_SIDE, MIN_SIDE),\n","            file_idx_offset=0\n","        )\n","    except Exception:\n","        # If Bing blocks this keyword, skip quietly\n","        time.sleep(0.5)\n","        continue\n","\n","    # Validate, deduplicate, move to Drive\n","    per_kw = 0\n","    for p in sorted(kw_dir.glob(\"*\")):\n","        if saved >= TARGET_TOTAL or per_kw >= MAX_PER_KEYWORD:\n","            break\n","        if not p.is_file():\n","            continue\n","\n","        try:\n","            img = Image.open(p).convert(\"RGB\")\n","            if min(img.size) < MIN_SIDE:\n","                continue\n","        except (UnidentifiedImageError, OSError):\n","            continue\n","\n","        try:\n","            h = sha1_file(p)\n","        except Exception:\n","            continue\n","        if h in existing_hashes:\n","            continue\n","\n","        out_name = f\"{q.replace(' ', '_')[:30]}_{h[:10]}.jpg\"\n","        out_path = OOD_DIR / out_name\n","        try:\n","            img.save(out_path, format=\"JPEG\", quality=90)\n","            existing_hashes.add(h)\n","            saved += 1\n","            per_kw += 1\n","        except Exception:\n","            continue\n","\n","    # small pause to avoid rate-limits\n","    time.sleep(0.5)\n","\n","print(f\"✅ Done. Total OOD images in Drive: {saved}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"yaUSkhDaoyok","executionInfo":{"status":"error","timestamp":1755987334726,"user_tz":-120,"elapsed":10181,"user":{"displayName":"Patricia Giménez","userId":"16444508660089000781"}},"outputId":"aab0f2af-9b7f-46b2-b451-ed12889fda11"},"id":"yaUSkhDaoyok","execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1198172272.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mexisting_hashes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msha1_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1198172272.py\u001b[0m in \u001b[0;36msha1_file\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msha1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8192\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1198172272.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msha1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8192\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# Create local folder where the dataset will be extracted\n","!mkdir -p /content/data"],"metadata":{"id":"pjl1NfXr569c"},"id":"pjl1NfXr569c","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Unzip dataset from Google Drive into Colab local storage\n","!unzip -q \"/content/drive/MyDrive/ecoscan/data/garbage_classification.zip\" -d /content/data/"],"metadata":{"id":"yHYMJwX459nX"},"id":"yHYMJwX459nX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load model & processor and rebuild a deterministic validation split ---\n","import os, json, math, random, pathlib\n","from pathlib import Path\n","import numpy as np\n","from PIL import Image\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","from sklearn.model_selection import train_test_split\n","\n","from transformers import AutoImageProcessor, ViTForImageClassification\n","\n","SEED = 42\n","random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n","\n","MODEL_DIR = Path(\"/content/drive/MyDrive/ecoscan/models/vit_ecoscan_v1\")\n","DATA_DIR  = Path(\"/content/data/garbage_classification\")       # local fast copy\n","OOD_DIR   = Path(\"/content/drive/MyDrive/ecoscan/data/ood_samples\")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","processor = AutoImageProcessor.from_pretrained(MODEL_DIR)\n","model      = ViTForImageClassification.from_pretrained(MODEL_DIR).to(device).eval()\n","\n","# Build class list from folders\n","IMG_EXTS   = {\".jpg\",\".jpeg\",\".png\",\".webp\",\".bmp\"}\n","IGNORE_DIR = {\".ipynb_checkpoints\",\"__MACOSX\"}\n","\n","classes = sorted([d.name for d in DATA_DIR.iterdir() if d.is_dir() and d.name not in IGNORE_DIR])\n","label2id = {c:i for i,c in enumerate(classes)}\n","id2label = {i:c for c,i in label2id.items()}\n","print(\"Classes:\", classes)\n","\n","# Collect all samples (path, label)\n","all_paths, all_labels = [], []\n","for c in classes:\n","    for p in (DATA_DIR / c).rglob(\"*\"):\n","        if p.is_file() and p.suffix.lower() in IMG_EXTS:\n","            all_paths.append(p)\n","            all_labels.append(label2id[c])\n","\n","# Stratified split (same approach as notebook 02)\n","train_paths, val_paths, y_train, y_val = train_test_split(\n","    all_paths, all_labels, test_size=0.2, random_state=SEED, stratify=all_labels\n",")\n","print(f\"Val size: {len(val_paths)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1vgDZAjq5YbE","executionInfo":{"status":"ok","timestamp":1755987401208,"user_tz":-120,"elapsed":29405,"user":{"displayName":"Patricia Giménez","userId":"16444508660089000781"}},"outputId":"968972af-8b52-41d3-a5be-fb68e149576b"},"id":"1vgDZAjq5YbE","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Classes: ['battery', 'brown-glass', 'cardboard', 'clothes', 'electronics', 'green-glass', 'metal_packaging', 'oil', 'organic', 'paper', 'plastic', 'shoes', 'tetrapak', 'trash', 'white-glass']\n","Val size: 3205\n"]}]},{"cell_type":"code","source":["#  Simple dataset using the saved processor\n","class ImgDataset(Dataset):\n","    def __init__(self, paths, labels=None):\n","        self.paths  = list(paths)\n","        self.labels = None if labels is None else list(labels)\n","\n","    def __len__(self): return len(self.paths)\n","\n","    def __getitem__(self, idx):\n","        img = Image.open(self.paths[idx]).convert(\"RGB\")\n","        enc = processor(images=img, return_tensors=\"pt\")\n","        item = {k: v.squeeze(0) for k,v in enc.items()}\n","        if self.labels is not None:\n","            item[\"labels\"] = torch.tensor(self.labels[idx]).long()\n","        item[\"path\"] = str(self.paths[idx])\n","        return item\n","\n","@torch.no_grad()\n","def predict_confidence(ds, batch_size=64):\n","    \"\"\"Return: logits (N,C), labels (or None), paths (N), max_prob (N), preds (N)\"\"\"\n","    dl = DataLoader(ds, batch_size=batch_size, shuffle=False, pin_memory=True)\n","    logits_list, labels_list, paths_list, maxp_list, pred_list = [], [], [], [], []\n","    for batch in tqdm(dl, desc=\"Infer\"):\n","        pixel_values = batch[\"pixel_values\"].to(device, non_blocking=True)\n","        outputs = model(pixel_values=pixel_values)\n","        logits = outputs.logits\n","        probs  = F.softmax(logits, dim=-1)\n","        maxp, preds = probs.max(dim=-1)\n","\n","        logits_list.append(logits.cpu())\n","        maxp_list.append(maxp.cpu())\n","        pred_list.append(preds.cpu())\n","        paths_list.extend(batch[\"path\"])\n","\n","        if \"labels\" in batch:\n","            labels_list.append(batch[\"labels\"].cpu())\n","\n","    logits = torch.cat(logits_list)\n","    maxp   = torch.cat(maxp_list).numpy()\n","    preds  = torch.cat(pred_list).numpy()\n","    labels = None if not labels_list else torch.cat(labels_list).numpy()\n","    return logits.numpy(), labels, np.array(paths_list), maxp, preds\n","\n","# Build datasets\n","val_ds = ImgDataset(val_paths, y_val)\n","\n","# OOD dataset (no labels)\n","ood_paths = [p for p in OOD_DIR.glob(\"*\") if p.is_file() and p.suffix.lower() in IMG_EXTS]\n","print(\"OOD images:\", len(ood_paths))\n","ood_ds = ImgDataset(ood_paths, labels=None)\n","\n","# Run inference\n","val_logits, val_labels, val_paths_arr, val_maxp, val_preds = predict_confidence(val_ds, batch_size=64)\n","ood_logits, _,          ood_paths_arr, ood_maxp, _         = predict_confidence(ood_ds, batch_size=64)\n","\n","# Quick sanity: in-domain accuracy (without any rejection)\n","id_top1 = (val_preds == val_labels).mean()\n","print(f\"In-domain (validation) top-1 accuracy (no reject): {id_top1:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3vDd43TV5eCN","executionInfo":{"status":"ok","timestamp":1755987459367,"user_tz":-120,"elapsed":41312,"user":{"displayName":"Patricia Giménez","userId":"16444508660089000781"}},"outputId":"1309c7b5-6fe1-4df5-fdb8-11781b4e679a"},"id":"3vDd43TV5eCN","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["OOD images: 180\n"]},{"output_type":"stream","name":"stderr","text":["Infer: 100%|██████████| 51/51 [00:18<00:00,  2.80it/s]\n","Infer: 100%|██████████| 3/3 [00:23<00:00,  7.68s/it]"]},{"output_type":"stream","name":"stdout","text":["In-domain (validation) top-1 accuracy (no reject): 0.9900\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["In this cell we built a tiny dataset wrapper that uses our saved image processor and ran the model to get predictions and their max softmax confidence for two groups: the validation set (known classes, with labels) and an OOD set (unknown classes, no labels). The OOD pool has 180 images. Inference ran fine (you see the progress bars), and on the validation set the model reached top‑1 accuracy = 0.99 without any rejection, which means it’s very good at recognizing images from the classes it was trained on. We did not measure accuracy on OOD (there are no labels); instead, we collected confidences that we’ll use next to pick a rejection threshold and see how well confidence separates “known” vs “unknown.” This gives us a clean baseline: strong in‑domain accuracy and confidence scores ready for calibration."],"metadata":{"id":"e0XBkK5x8YOz"},"id":"e0XBkK5x8YOz"},{"cell_type":"code","source":["# Threshold search: maximize macro-F1 over (ID accepted & correct) and (OOD rejected)\n","\n","def evaluate_threshold(t, val_maxp, val_labels, val_preds, ood_maxp):\n","    # Accept in-domain only if (conf >= t) AND correct\n","    id_accept = (val_maxp >= t) & (val_preds == val_labels)\n","    id_reject = ~id_accept  # either wrong or low confidence\n","\n","    # For OOD, we want to reject if conf < t\n","    ood_reject = (ood_maxp < t)\n","    ood_accept = ~ood_reject\n","\n","    # Confusion-style counts for two “good” classes:\n","    # Class A: ID_correct_accepted    | positive when id_accept True\n","    # Class B: OOD_rejected           | positive when ood_reject True\n","    A_tp = id_accept.sum()\n","    A_fp = ood_accept.sum()   # OOD accepted looks like false acceptance for A\n","    A_fn = (~id_accept).sum()\n","\n","    B_tp = ood_reject.sum()\n","    B_fp = id_accept.sum()    # accepted ID correct looks like false positive for B\n","    B_fn = (~ood_reject).sum()\n","\n","    def f1(tp, fp, fn):\n","        prec = tp / (tp + fp + 1e-9)\n","        rec  = tp / (tp + fn + 1e-9)\n","        return 2*prec*rec/(prec+rec+1e-9)\n","\n","    f1_A = f1(A_tp, A_fp, A_fn)\n","    f1_B = f1(B_tp, B_fp, B_fn)\n","    macro_f1 = (f1_A + f1_B)/2.0\n","    return macro_f1, {\n","        \"id_accept\": int(A_tp), \"id_total\": int(len(val_maxp)),\n","        \"ood_reject\": int(B_tp), \"ood_total\": int(len(ood_maxp)),\n","        \"f1_idaccept\": float(f1_A), \"f1_oodreject\": float(f1_B)\n","    }\n","\n","ts = np.linspace(0.50, 0.999, 200)   # search region; ViT probs are usually high\n","best = (-1, None, None)\n","for t in ts:\n","    score, info = evaluate_threshold(t, val_maxp, val_labels, val_preds, ood_maxp)\n","    if score > best[0]:\n","        best = (score, t, info)\n","\n","best_score, best_t, best_info = best\n","print(f\"Best macro-F1: {best_score:.4f} at threshold {best_t:.3f}\")\n","print(best_info)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8LLN_I-m6p3x","executionInfo":{"status":"ok","timestamp":1755987467056,"user_tz":-120,"elapsed":34,"user":{"displayName":"Patricia Giménez","userId":"16444508660089000781"}},"outputId":"1ed51d9f-7943-49ce-c686-b7f9308efdb5"},"id":"8LLN_I-m6p3x","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Best macro-F1: 0.5313 at threshold 0.831\n","{'id_accept': 3095, 'id_total': 3205, 'ood_reject': 146, 'ood_total': 180, 'f1_idaccept': 0.9772655504943956, 'f1_oodreject': 0.08535515921039788}\n"]}]},{"cell_type":"markdown","source":["Here we tried many confidence thresholds to see where the model best separates known vs unknown images.\n","\n","The best point is at threshold ≈ 0.83.\n","\n","With this, the model accepts 3095 / 3205 validation images correctly.\n","\n","It also rejects 146 / 180 OOD images.\n","\n","So: the model is great at keeping correct known images (F1 ≈ 0.98), but still weak at rejecting unknowns (F1 ≈ 0.09)."],"metadata":{"id":"sZLj7sOo87hP"},"id":"sZLj7sOo87hP"},{"cell_type":"code","source":["# Tiny Outlier Exposure (OE) fine-tune — single cell, quick & safe\n","\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","import math, itertools\n","\n","# 1) Use train_ds if you have it; otherwise fall back to val_ds\n","id_ds = train_ds if 'train_ds' in globals() else val_ds\n","\n","# 2) Build small OOD dataset & loaders\n","class OODDataset(Dataset):\n","    def __init__(self, paths):\n","        self.paths = list(paths)\n","    def __len__(self): return len(self.paths)\n","    def __getitem__(self, idx):\n","        img = Image.open(self.paths[idx]).convert(\"RGB\")\n","        enc = processor(images=img, return_tensors=\"pt\")\n","        item = {k: v.squeeze(0) for k,v in enc.items()}\n","        return item\n","\n","ood_ds_small = OODDataset(ood_paths)  # we already have ood_paths from before\n","\n","batch_size_id  = 32\n","batch_size_ood = 32\n","id_loader  = DataLoader(id_ds,  batch_size=batch_size_id, shuffle=True,  pin_memory=True)\n","ood_loader = DataLoader(ood_ds_small, batch_size=batch_size_ood, shuffle=True, pin_memory=True)\n","\n","# 3) Optimizer (tiny LR, short run)\n","model.train()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6, weight_decay=0.01)\n","\n","# 4) One short epoch with OE loss\n","num_classes = model.config.num_labels\n","lambda_oe   = 0.5     # strength of OE regularization (0.3–1.0 is typical)\n","max_steps   = math.ceil(len(id_loader))  # ~1 epoch over ID data\n","\n","ood_iter = itertools.cycle(ood_loader)\n","pbar = tqdm(range(max_steps), desc=\"OE fine-tune (1 epoch)\")\n","\n","for _ in pbar:\n","    # ----- In-domain supervised batch -----\n","    id_batch = next(iter(id_loader))  # grab next supervised batch\n","    pixel_values = id_batch[\"pixel_values\"].to(device, non_blocking=True)\n","    labels       = id_batch[\"labels\"].to(device, non_blocking=True)\n","\n","    out_id = model(pixel_values=pixel_values)\n","    loss_id = F.cross_entropy(out_id.logits, labels)\n","\n","    # ----- OOD batch with uniform target (make model uncertain) -----\n","    ood_batch = next(ood_iter)\n","    ood_pixels = ood_batch[\"pixel_values\"].to(device, non_blocking=True)\n","    out_ood = model(pixel_values=ood_pixels)\n","\n","    # Cross-entropy to a uniform distribution = - (1/C) * sum log p_i\n","    log_probs = F.log_softmax(out_ood.logits, dim=-1)\n","    loss_ood  = -(log_probs.mean(dim=1)).mean()  # equivalent to uniform target\n","\n","    # ----- Total loss & update -----\n","    loss = loss_id + lambda_oe * loss_ood\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n","\n","    pbar.set_postfix({'loss_id': float(loss_id.item()), 'loss_ood': float(loss_ood.item())})\n","\n","model.eval()\n","print(\"Done. Now re-run your calibration cell to pick a new threshold.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y9lYSQmM-fO6","executionInfo":{"status":"ok","timestamp":1755987529908,"user_tz":-120,"elapsed":59074,"user":{"displayName":"Patricia Giménez","userId":"16444508660089000781"}},"outputId":"d24dc956-a92b-49ca-af97-619013436e98"},"id":"Y9lYSQmM-fO6","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["OE fine-tune (1 epoch): 100%|██████████| 101/101 [00:59<00:00,  1.71it/s, loss_id=0.0236, loss_ood=2.89]"]},{"output_type":"stream","name":"stdout","text":["Done. Now re-run your calibration cell to pick a new threshold.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["#  Improved OOD calibration: MSP vs MaxLogit vs Energy (with temperature grid)\n","\n","def energy_score(logits, T=1.0):\n","    # E(x) = -T * logsumexp(logits / T)\n","    x = logits / T\n","    m = x.max(axis=1, keepdims=True)\n","    lse = m + np.log(np.exp(x - m).sum(axis=1, keepdims=True))\n","    return (-T * lse.squeeze())\n","\n","# Build alternative scores\n","val_msp = val_maxp                      # higher = more in-domain\n","ood_msp = ood_maxp\n","\n","val_maxlogit = val_logits.max(axis=1)   # higher = more in-domain\n","ood_maxlogit = ood_logits.max(axis=1)\n","\n","temps = [0.5, 1.0, 2.0, 5.0]\n","energy_sets = {}\n","for T in temps:\n","    energy_sets[T] = (\n","        energy_score(val_logits, T=T),  # lower = more in-domain\n","        energy_score(ood_logits, T=T)\n","    )\n","\n","def eval_with_scores(scores_val, scores_ood, higher_is_better=True, ts=None):\n","    # choose a sensible threshold search range\n","    if ts is None:\n","        both = np.concatenate([scores_val, scores_ood])\n","        lo, hi = np.percentile(both, 1), np.percentile(both, 99)\n","        ts = np.linspace(lo, hi, 200)\n","\n","    best = (-1, None, None)\n","    for t in ts:\n","        if higher_is_better:\n","            id_accept = (scores_val >= t) & (val_preds == val_labels)\n","            ood_reject = (scores_ood <  t)\n","        else:\n","            id_accept = (scores_val <= t) & (val_preds == val_labels)\n","            ood_reject = (scores_ood >  t)\n","\n","        A_tp = id_accept.sum()\n","        A_fp = (~ood_reject).sum()  # OOD accepted\n","        A_fn = (~id_accept).sum()\n","\n","        B_tp = ood_reject.sum()\n","        B_fp = id_accept.sum()\n","        B_fn = (~ood_reject).sum()\n","\n","        def f1(tp, fp, fn):\n","            prec = tp / (tp + fp + 1e-9)\n","            rec  = tp / (tp + fn + 1e-9)\n","            return 2*prec*rec/(prec+rec+1e-9)\n","\n","        f1_A = f1(A_tp, A_fp, A_fn)\n","        f1_B = f1(B_tp, B_fp, B_fn)\n","        macro_f1 = (f1_A + f1_B)/2.0\n","\n","        info = {\n","            \"id_accept\": int(A_tp), \"id_total\": int(len(scores_val)),\n","            \"ood_reject\": int(B_tp), \"ood_total\": int(len(scores_ood)),\n","            \"f1_idaccept\": float(f1_A), \"f1_oodreject\": float(f1_B)\n","        }\n","        if macro_f1 > best[0]:\n","            best = (macro_f1, float(t), info)\n","    return best  # (score, t, info)\n","\n","candidates = []\n","\n","# MSP\n","candidates.append((\"msp\",) + eval_with_scores(val_msp, ood_msp, higher_is_better=True))\n","\n","# MaxLogit\n","candidates.append((\"maxlogit\",) + eval_with_scores(val_maxlogit, ood_maxlogit, higher_is_better=True))\n","\n","# Energy (note: lower = more in-domain)\n","for T, (v_e, o_e) in energy_sets.items():\n","    score, t, info = eval_with_scores(v_e, o_e, higher_is_better=False)\n","    candidates.append((f\"energy_T{T}\", score, t, info))\n","\n","# Pick best method\n","best_method, best_score, best_t, best_info = max(candidates, key=lambda x: x[1])\n","print(\"== Best calibration ==\")\n","print(f\"Method: {best_method}\")\n","print(f\"Best macro-F1: {best_score:.4f} at threshold {best_t:.6f}\")\n","print(best_info)\n","\n","# Save improved threshold & method\n","EVAL_DIR = MODEL_DIR / \"eval\"\n","EVAL_DIR.mkdir(parents=True, exist_ok=True)\n","\n","th_json = {\n","    \"method\": best_method,\n","    \"reject_threshold\": float(best_t),\n","    \"rule\": (\"accept if score >= t (MSP/MaxLogit)\"\n","             if best_method in [\"msp\", \"maxlogit\"] else\n","             \"accept if energy <= t\"),\n","    \"note\": \"If rule not met, treat as OOD (reject).\",\n","    \"calibration_samples\": {\n","        \"in_domain_val\": int(len(val_msp)),\n","        \"ood\": int(len(ood_msp))\n","    },\n","    \"metrics_at_threshold\": best_info,\n","}\n","with open(EVAL_DIR / \"reject_threshold.json\", \"w\") as f:\n","    json.dump(th_json, f, indent=2)\n","\n","print(\"Saved:\", EVAL_DIR / \"reject_threshold.json\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qm8_FRnh-84p","executionInfo":{"status":"ok","timestamp":1755987538373,"user_tz":-120,"elapsed":680,"user":{"displayName":"Patricia Giménez","userId":"16444508660089000781"}},"outputId":"f3eca48a-d1ea-4137-b69a-165c9d9ae802"},"id":"qm8_FRnh-84p","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["== Best calibration ==\n","Method: energy_T1.0\n","Best macro-F1: 0.5348 at threshold -4.697142\n","{'id_accept': 3136, 'id_total': 3205, 'ood_reject': 148, 'ood_total': 180, 'f1_idaccept': 0.9841518902889751, 'f1_oodreject': 0.08545034632174821}\n","Saved: /content/drive/MyDrive/ecoscan/models/vit_ecoscan_v1/eval/reject_threshold.json\n"]}]},{"cell_type":"code","source":["# Choose an operating point: target OOD rejection (simple & robust)\n","\n","TARGET_OOD_REJECT = 0.75  # try 0.90, 0.95, or 0.80 depending on your needs\n","\n","def energy_score_np(logits, T=1.0):\n","    x = logits / T\n","    m = x.max(axis=1, keepdims=True)\n","    lse = m + np.log(np.exp(x - m).sum(axis=1, keepdims=True))\n","    return (-T * lse.squeeze())  # lower = more in-domain\n","\n","# Compute Energy(T=1.0) from the logits you already have\n","val_energy = energy_score_np(val_logits, T=1.0)\n","ood_energy = energy_score_np(ood_logits, T=1.0)\n","\n","# We accept if energy <= t (in-domain), reject if energy > t (OOD)\n","ts = np.linspace(np.percentile(np.concatenate([val_energy, ood_energy]), 1),\n","                 np.percentile(np.concatenate([val_energy, ood_energy]), 99), 300)\n","\n","best = None\n","for t in ts:\n","    id_accept = (val_energy <= t) & (val_preds == val_labels)\n","    ood_reject = (ood_energy >  t)\n","\n","    ood_reject_rate = ood_reject.mean()\n","    if ood_reject_rate >= TARGET_OOD_REJECT:\n","        # among feasible thresholds, maximize ID accepted & correct\n","        score = id_accept.sum()\n","        info = {\n","            \"threshold\": float(t),\n","            \"id_accept\": int(id_accept.sum()),\n","            \"id_total\": int(len(val_energy)),\n","            \"ood_reject\": int(ood_reject.sum()),\n","            \"ood_total\": int(len(ood_energy)),\n","            \"ood_reject_rate\": float(ood_reject_rate)\n","        }\n","        if (best is None) or (score > best[0]):\n","            best = (score, info)\n","\n","if best is None:\n","    # if the target is too strict, fall back to the best achievable point\n","    # (highest OOD rejection; tie-breaker = more ID accepted)\n","    best = (-1, None)\n","    for t in ts:\n","        id_accept = (val_energy <= t) & (val_preds == val_labels)\n","        ood_reject = (ood_energy >  t)\n","        cand = (ood_reject.mean(), id_accept.sum(), float(t), id_accept, ood_reject)\n","        if best[1] is None or (cand[0] > best[1][\"ood_reject_rate\"]) or \\\n","           (cand[0] == best[1][\"ood_reject_rate\"] and cand[1] > best[0]):\n","            best = (cand[1], {\n","                \"threshold\": cand[2],\n","                \"id_accept\": int(cand[1]),\n","                \"id_total\": int(len(val_energy)),\n","                \"ood_reject\": int(cand[4].sum()),\n","                \"ood_total\": int(len(ood_energy)),\n","                \"ood_reject_rate\": float(cand[0])\n","            })\n","\n","best_info = best[1]\n","print(\"== Operating point (Energy, T=1.0) ==\")\n","print(f\"Target OOD reject: {TARGET_OOD_REJECT*100:.0f}%\")\n","print(f\"Chosen threshold: {best_info['threshold']:.6f}\")\n","print(f\"ID accepted & correct: {best_info['id_accept']} / {best_info['id_total']}\")\n","print(f\"OOD rejected: {best_info['ood_reject']} / {best_info['ood_total']} \"\n","      f\"({best_info['ood_reject_rate']*100:.1f}%)\")\n","\n","# Save to JSON (for your demo)\n","EVAL_DIR = MODEL_DIR / \"eval\"\n","EVAL_DIR.mkdir(parents=True, exist_ok=True)\n","import json\n","json.dump({\n","    \"method\": \"energy_T1.0_target_ood_reject\",\n","    \"reject_threshold\": best_info[\"threshold\"],\n","    \"target_ood_reject\": TARGET_OOD_REJECT,\n","    \"rule\": \"accept if energy <= threshold; else reject\",\n","    \"metrics_at_threshold\": best_info\n","}, open(EVAL_DIR / \"reject_threshold.json\", \"w\"), indent=2)\n","print(\"Saved:\", EVAL_DIR / \"eval\" / \"reject_threshold.json\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IvZdOvlYAaNv","executionInfo":{"status":"ok","timestamp":1755987767542,"user_tz":-120,"elapsed":33,"user":{"displayName":"Patricia Giménez","userId":"16444508660089000781"}},"outputId":"d13cf706-6f68-4a4d-e6cd-ed469d60838b"},"id":"IvZdOvlYAaNv","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["== Operating point (Energy, T=1.0) ==\n","Target OOD reject: 75%\n","Chosen threshold: -4.222049\n","ID accepted & correct: 3155 / 3205\n","OOD rejected: 138 / 180 (76.7%)\n","Saved: /content/drive/MyDrive/ecoscan/models/vit_ecoscan_v1/eval/eval/reject_threshold.json\n"]}]},{"cell_type":"code","source":["#Compare operating points for multiple OOD-rejection targets\n","import numpy as np, json\n","\n","def energy_score_np(logits, T=1.0):\n","    x = logits / T\n","    m = x.max(axis=1, keepdims=True)\n","    lse = m + np.log(np.exp(x - m).sum(axis=1, keepdims=True))\n","    return (-T * lse.squeeze())  # lower = more in-domain\n","\n","val_energy = energy_score_np(val_logits, T=1.0)\n","ood_energy = energy_score_np(ood_logits, T=1.0)\n","\n","def pick_threshold_for_target(target):\n","    both = np.concatenate([val_energy, ood_energy])\n","    ts = np.linspace(np.percentile(both, 1), np.percentile(both, 99), 500)\n","\n","    best = None\n","    for t in ts:\n","        id_accept = (val_energy <= t) & (val_preds == val_labels)\n","        ood_reject = (ood_energy >  t)\n","        if ood_reject.mean() >= target:\n","            # maximize correct ID accepts among feasible thresholds\n","            score = id_accept.sum()\n","            info = {\n","                \"threshold\": float(t),\n","                \"id_accept\": int(id_accept.sum()),\n","                \"id_total\": int(len(val_energy)),\n","                \"ood_reject\": int(ood_reject.sum()),\n","                \"ood_total\": int(len(ood_energy)),\n","                \"ood_reject_rate\": float(ood_reject.mean()),\n","                \"id_accept_rate\": float(id_accept.mean())\n","            }\n","            if (best is None) or (score > best[0]):\n","                best = (score, info)\n","    # if none meets the target, return best achievable\n","    if best is None:\n","        best_rate = -1.0; best = None\n","        for t in ts:\n","            id_accept = (val_energy <= t) & (val_preds == val_labels)\n","            ood_reject = (ood_energy >  t)\n","            rate = ood_reject.mean()\n","            if rate > best_rate or (rate == best_rate and id_accept.sum() > best[0]):\n","                best_rate = rate\n","                best = (id_accept.sum(), {\n","                    \"threshold\": float(t),\n","                    \"id_accept\": int(id_accept.sum()),\n","                    \"id_total\": int(len(val_energy)),\n","                    \"ood_reject\": int(ood_reject.sum()),\n","                    \"ood_total\": int(len(ood_energy)),\n","                    \"ood_reject_rate\": float(rate),\n","                    \"id_accept_rate\": float(id_accept.mean())\n","                })\n","    return best[1]\n","\n","targets = [0.75, 0.78, 0.80, 0.85, 0.90, 0.95]\n","rows = []\n","for tgt in targets:\n","    info = pick_threshold_for_target(tgt)\n","    rows.append((tgt, info))\n","\n","print(\"== Operating points (Energy T=1.0) ==\")\n","for tgt, info in rows:\n","    print(f\"Target {int(tgt*100)}% -> thr {info['threshold']:.4f} | \"\n","          f\"ID accepted {info['id_accept']}/{info['id_total']} ({info['id_accept_rate']*100:.1f}%) | \"\n","          f\"OOD rejected {info['ood_reject']}/{info['ood_total']} ({info['ood_reject_rate']*100:.1f}%)\")\n","\n","# Save your chosen point (pick one target below)\n","CHOSEN_TARGET = 0.75  # <- change to 0.80 / 0.85 / 0.90 / 0.95 as you prefer\n","chosen_info = [info for tgt, info in rows if abs(tgt-CHOSEN_TARGET) < 1e-9][0]\n","\n","EVAL_DIR = MODEL_DIR / \"eval\"\n","EVAL_DIR.mkdir(parents=True, exist_ok=True)\n","with open(EVAL_DIR / \"reject_threshold.json\", \"w\") as f:\n","    json.dump({\n","        \"method\": \"energy_T1.0_target_ood_reject\",\n","        \"reject_threshold\": chosen_info[\"threshold\"],\n","        \"target_ood_reject\": CHOSEN_TARGET,\n","        \"rule\": \"accept if energy <= threshold; else reject\",\n","        \"metrics_at_threshold\": chosen_info\n","    }, f, indent=2)\n","\n","print(\"Saved:\", EVAL_DIR / \"reject_threshold.json\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GlXhn6_yBLGG","executionInfo":{"status":"ok","timestamp":1755987573703,"user_tz":-120,"elapsed":102,"user":{"displayName":"Patricia Giménez","userId":"16444508660089000781"}},"outputId":"62b5db1b-059e-4cab-cd03-6513de9a765f"},"id":"GlXhn6_yBLGG","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["== Operating points (Energy T=1.0) ==\n","Target 75% -> thr -4.1559 | ID accepted 3158/3205 (98.5%) | OOD rejected 135/180 (75.0%)\n","Target 78% -> thr -4.3643 | ID accepted 3151/3205 (98.3%) | OOD rejected 142/180 (78.9%)\n","Target 80% -> thr -4.3792 | ID accepted 3149/3205 (98.3%) | OOD rejected 144/180 (80.0%)\n","Target 85% -> thr -5.4664 | ID accepted 3072/3205 (95.9%) | OOD rejected 153/180 (85.0%)\n","Target 90% -> thr -6.6578 | ID accepted 2838/3205 (88.5%) | OOD rejected 162/180 (90.0%)\n","Target 95% -> thr -7.5811 | ID accepted 2354/3205 (73.4%) | OOD rejected 171/180 (95.0%)\n","Saved: /content/drive/MyDrive/ecoscan/models/vit_ecoscan_v1/eval/reject_threshold.json\n"]}]},{"cell_type":"markdown","source":["We compared several operating points using the Energy score (T=1.0). At 75% OOD rejection the model keeps 98% of validation images but misses too many unknowns. At 90–95% it becomes stricter but loses too much in-domain coverage (down to 88% or even 73%). We finally selected the 85% OOD rejection point: it rejects 153 / 180 unknown images (85%) while still accepting 3072 / 3205 validation images (96%). This gives a clear and professional balance between safety and usability."],"metadata":{"id":"a_x7hqoEB0VG"},"id":"a_x7hqoEB0VG"}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}